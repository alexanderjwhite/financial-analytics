{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Housing Prices Using Linear Regression\n",
    "\n",
    "In this notebook, we'll apply gradient descent to solve a simple linear regression problem: predicting housing prices based on the size of a house. By gradually adjusting the model's parameters (slope and intercept), we’ll minimize the error between the predicted and actual prices, making this a real-world example of optimization in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Setup\n",
    "\n",
    "We’ll use a simple, simulated dataset with `size` as the independent variable and `price` as the target variable. The goal is to find a line that best fits the data, using gradient descent to minimize the **Mean Squared Error (MSE)** between our predictions and the actual prices.\n",
    "\n",
    "The linear relationship we’re trying to model is:\n",
    "\n",
    "$\\text{price} = w \\times \\text{size} + b$\n",
    "\n",
    "where:\n",
    "- $w$ is the slope (how much price increases per square meter),\n",
    "- $b$ is the intercept (base price when the size is zero).\n",
    "\n",
    "## 2. Cost Function: Mean Squared Error (MSE)\n",
    "\n",
    "To measure how well our model fits the data, we’ll use the Mean Squared Error (MSE):\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - (\\hat{w} x_i + \\hat{b}) \\right)^2$\n",
    "where:\n",
    "- $y_i$ is the actual price,\n",
    "- $x_i$ is the size,\n",
    "- $\\hat{w}$ and  $\\hat{b}$ are the model parameters (slope and intercept).\n",
    "\n",
    "Our goal is to minimize the MSE by adjusting $w$ and $b$ over a series of iterations.\n",
    "\n",
    "## 3. Gradient Descent Algorithm\n",
    "\n",
    "**Gradient descent** is an optimization algorithm that iteratively adjusts the parameters to minimize the cost function (MSE, in this case). At each step, we compute the **gradient** of the MSE with respect to $\\hat{w}$ and $\\hat{b}$, then update these parameters in the opposite direction of the gradient to reduce the error.\n",
    "\n",
    "The update rules are:\n",
    "$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{MSE}}{\\partial w}$\n",
    "\n",
    "$b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial \\text{MSE}}{\\partial b}$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **learning rate**—a small positive number that controls the size of the update steps.\n",
    "\n",
    "In each iteration, we:\n",
    "1. Predict the prices using the current values of $w$ and $b$.\n",
    "2. Calculate the MSE based on these predictions.\n",
    "3. Update $w$ and $b$ using the gradients to reduce the MSE.\n",
    "\n",
    "## 4. Visualization of the Optimization Process\n",
    "\n",
    "To help visualize the optimization, we’ll plot the regression line at selected iterations (every 5th iteration), so we can see how the line moves closer to the actual data as gradient descent progresses. Additionally, we'll plot the MSE over each iteration to show how the error decreases over time.\n",
    "\n",
    "## 5. Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple dataset (e.g., house size vs. price)\n",
    "np.random.seed(0)\n",
    "size = np.random.rand(100) * 100  # Size of the house in square meters\n",
    "true_w, true_b = 5, 10  # Assume true relationship: price = 5 * size + 10\n",
    "price = true_w * size + true_b + np.random.randn(100) * 10  # Adding some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 0, 0  # Initial guesses for slope and intercept\n",
    "\n",
    "# Store the values for plotting\n",
    "w_values, b_values = [w], [b]\n",
    "mse_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = w * size + b\n",
    "# Calculate Mean Squared Error\n",
    "mse = np.mean((price - predictions) ** 2)\n",
    "mse_values.append(mse)\n",
    "\n",
    "# Derivatives\n",
    "w_grad = -2 * np.mean(size * (price - predictions))\n",
    "b_grad = -2 * np.mean(price - predictions)\n",
    "\n",
    "# Update parameters\n",
    "w -= alpha * w_grad\n",
    "b -= alpha * b_grad\n",
    "\n",
    "print(\"w = \",w, \"MSE = \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Parameters\n",
    "alpha = 0.00002  # Learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "w, b = 0, 0  # Initial guesses for slope and intercept\n",
    "\n",
    "# Store the values for plotting\n",
    "w_values, b_values = [w], [b]\n",
    "mse_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the values for plotting\n",
    "w_values, b_values = [w], [b]\n",
    "mse_values = []\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for _ in range(iterations):\n",
    "    # Predicted prices with current w, b\n",
    "    predictions = w * size + b\n",
    "    # Calculate Mean Squared Error\n",
    "    mse = np.mean((price - predictions) ** 2)\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Derivatives\n",
    "    w_grad = -2 * np.mean(size * (price - predictions))\n",
    "    b_grad = -2 * np.mean(price - predictions)\n",
    "    \n",
    "    # Update parameters\n",
    "    w -= alpha * w_grad\n",
    "    b -= alpha * b_grad\n",
    "    \n",
    "    # Store parameters for visualization\n",
    "    w_values.append(w)\n",
    "    b_values.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the optimization process\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(size, price, color='blue', label='Data (Size vs Price)')\n",
    "\n",
    "# Plot the regression line for each iteration\n",
    "for i in range(0, len(w_values), 10):  # Plot every 10th iteration\n",
    "    plt.plot(size, w_values[i] * size + b_values[i], \n",
    "             label=f\"Iteration {i}\", alpha=0.5)\n",
    "\n",
    "# Final regression line\n",
    "plt.plot(size, w * size + b, color='red', label='Final Regression Line', linewidth=2)\n",
    "plt.xlabel(\"Size (Square meters)\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Gradient Descent Optimization for Housing Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MSE over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mse_values, color='purple')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE Reduction Over Iterations\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
